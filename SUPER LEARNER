# Load necessary libraries; install if not already installed
required_packages <- c("caret", "dplyr", "lubridate", "readr", "SuperLearner", "ggplot2", "nnet", "randomForest", "xgboost")
new_packages <- required_packages[!(required_packages %in% installed.packages()[, "Package"])]
if(length(new_packages)) {
  install.packages(new_packages, dependencies = TRUE)
}
invisible(lapply(required_packages, library, character.only = TRUE))

# Function to create Fourier terms for seasonality
create_fourier_terms <- function(data, time_var, K, period) {
  for (k in 1:K) {
    data <- mutate(data, !!paste0("sin_", k) := sin(2 * pi * k * !!sym(time_var) / period),
                   !!paste0("cos_", k) := cos(2 * pi * k * !!sym(time_var) / period))
  }
  return(data)
}

# Load and preprocess data
files <- c(
  "/kaggle/input/final-merged-dataset/AAPL_merged_data (1).csv" = "AAPL",
  "/kaggle/input/final-merged-dataset/AMZN_merged_data (1).csv" = "AMZN",
  "/kaggle/input/final-merged-dataset/GOOG_merged_data (1).csv" = "GOOG",
  "/kaggle/input/final-merged-dataset/MSFT_merged_data (1).csv" = "MSFT",
  "/kaggle/input/final-merged-dataset/TSLA_merged_data (1).csv" = "TSLA"
)

data_list <- lapply(names(files), function(file) {
  data <- read_csv(file, col_types = cols())
  mutate(data, Ticker = files[[file]])
})

full_dataset <- bind_rows(data_list) %>%
  mutate(Date = ymd(Date)) %>%
  arrange(Date, Ticker) %>%
  group_by(Ticker) %>%
  mutate(across(c("RSI", "Volatility"), list(lag1 = lag, lag2 = ~lag(.x, 2), lag3 = ~lag(.x, 3))),
         Excess_Return = diff(c(NA, Excess_Return)),
         Excess_Return_lag1 = lag(Excess_Return, 1),
         Excess_Return_lag2 = lag(Excess_Return, 2),
         Excess_Return_lag3 = lag(Excess_Return, 3),
         Year = year(Date), Quarter = quarter(Date), Month = month(Date), Week = week(Date), DayOfWeek = wday(Date), DayOfYear = yday(Date)) %>%
  ungroup()

# Add Fourier terms for annual seasonality
full_dataset <- create_fourier_terms(full_dataset, "DayOfYear", K = 3, period = 252)

# Standardize column names
names(full_dataset) <- gsub(" ", ".", names(full_dataset))

# Define features before subsetting data
features <- c("Avg_Volume", "RSI", "Volatility", "Excess_Return_lag1", "Excess_Return_lag2", "Excess_Return_lag3", paste0("sin_", 1:3), paste0("cos_", 1:3))

# Prepare data for modeling
split_date <- as.Date("2019-01-01")
train_data <- filter(full_dataset, Date < split_date)
test_data <- filter(full_dataset, Date >= split_date)

# Subsetting train_data and test_data
train_data <- train_data[21:nrow(train_data), ]
test_data <- test_data[21:nrow(test_data), ]

# Ensure all features are present in the training data
if (!all(features %in% names(train_data))) {
  missing_features <- features[!features %in% names(train_data)]
  stop("The following features are missing in the training data: ", paste(missing_features, collapse = ", "))
}

# Apply transformations
train_data[,"Rolling.Sentiment"] <- scale(train_data[,"Rolling.Sentiment"])
test_data[,"Rolling.Sentiment"] <- scale(test_data[,"Rolling.Sentiment"])

# Create interaction terms
train_data[,"Rolling_Sentiment_Avg_Volume"] <- train_data[,"Rolling.Sentiment"] * train_data[,"Avg_Volume"]
test_data[,"Rolling_Sentiment_Avg_Volume"] <- test_data[,"Rolling.Sentiment"] * test_data[,"Avg_Volume"]

# Update features
features <- c(features, "Rolling_Sentiment_Avg_Volume")

# Prepare train and test features
train_features <- train_data[, features]
test_features <- test_data[, features]

# Configure and train the SuperLearner model
super_model <- SuperLearner(
  Y = train_data$Excess_Return,
  X = train_features,
  SL.library = c("SL.glmnet", "SL.randomForest", "SL.nnls", "SL.xgboost"),
  method = "method.NNLS",
  cvControl = list(V = 5),
  verbose = TRUE
)

# Prediction and plotting
if (!is.null(super_model)) {
  predictions_super <- predict(super_model, newdata = test_features, type = "response")$pred
  test_data <- mutate(test_data, Predicted_Return = predictions_super)
  print(
    ggplot(test_data, aes(x = Date, y = Excess_Return, colour = "Actual")) +
      geom_line() +
      geom_line(aes(y = Predicted_Return, colour = "Predicted")) +
      scale_colour_manual(values = c("Actual" = "blue", "Predicted" = "red")) +
      labs(title = "Actual vs Predicted Excess Returns Over Time", x = "Date", y = "Excess Return") +
      theme_minimal()
  )
} else {
  warning("SuperLearner model could not be created or is NULL.")
}

# Evaluation metrics
rss <- sum((test_data$Excess_Return - test_data$Predicted_Return)^2)
tss <- sum((test_data$Excess_Return)^2)
R2 <- 1 - rss / tss
n <- nrow(test_data)
p <- length(features)
R2_adj <- 1 - (1 - R2) * (n - 1) / (n - p - 1)

# Print evaluation metrics
cat("Out-of-Sample R-squared:", R2, "\n")
cat("Adjusted R-squared:", R2_adj, "\n")
cat("Correlation:", cor(test_data$Predicted_Return, test_data$Excess_Return, use = "complete.obs"), "\n")
cat("RMSE:", sqrt(mean((test_data$Predicted_Return - test_data$Excess_Return)^2, na.rm = TRUE)), "\n")
cat("MSE:", mean((test_data$Predicted_Return - test_data$Excess_Return)^2, na.rm = TRUE), "\n")
